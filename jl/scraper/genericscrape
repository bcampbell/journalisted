#!/usr/bin/env python
""" commandline tool to grab parse article text and metadata from a given url """

import urllib2
import logging
from datetime import datetime,timedelta
from optparse import OptionParser

try:
    import simplejson as json
except ImportError:
    import json

import metareadability
import decruft

import site
site.addsitedir("../pylib")
from JL import ScraperUtils
from JL import ukmedia



def context_from_url(url):
    context = {}
    context['permalink']=url
    context['srcurl']=url
    context['lastseen'] = datetime.now()
    return context

def extract(html,context):
    art = context

    headline,byline,pubdate = metareadability.extract(html,context['srcurl'])
    if headline is not None:
        art['title'] = headline
    if pubdate is not None:
        art['pubdate'] = pubdate
    if byline is not None:
        art['byline'] = byline
    else:
        art['byline'] = u''

    txt = decruft.Document(html).summary()
    art['content'] = ukmedia.SanitiseHTML(txt)
    return art


def main():
    max_errors = 100

    parser = OptionParser(usage="%prog: [options] urls")
    parser.add_option('-v', '--verbose', action='store_true')
    parser.add_option('-d', '--debug', action='store_true')
    parser.add_option('-t', '--test', action='store_true', help="test only - don't commit to db")
    parser.add_option('-f', '--force_rescrape', action='store_true')
    parser.add_option('-s', '--source_feeds', dest="source_feeds", help="list of rss feeds, file in json format")
    parser.add_option("-m", "--max_errors", type="int", default=max_errors, help="set num of errors allowed before quitting (default %d)" % (max_errors,))
    parser.add_option("-a", "--max_age", type="int", default=None, help="discard any articles older than x hours old (only if feeds -s is used)")
    (options, args) = parser.parse_args()

    log_level = logging.ERROR
    if options.debug:
        log_level = logging.DEBUG
    if options.verbose:
        log_level = logging.INFO

    logging.basicConfig(level=log_level, format='%(message)s')

    if options.source_feeds:
        feeds = json.loads(open(options.source_feeds, "rt").read())

        arts = ScraperUtils.FindArticlesFromRSS([(f[1],f[0]) for f in feeds], None, None, maxerrors=20 )

        if options.max_age is not None:
            cutoff = datetime.now() - timedelta(hours=options.max_age)

            logging.info("discarding anything before %s" % (cutoff))
            logging.info("before cutoff: %d" %(len(arts),))
            arts = [a for a in arts if a['pubdate']>cutoff]
            logging.info("after cutoff: %d" %(len(arts),))


        ScraperUtils.scrape_articles(arts, extract, options)


    else:
        # individual urls
        arts = [context_from_url(url) for url in args]

        ScraperUtils.scrape_articles(arts, extract, options)

if __name__ == '__main__':
    main()

