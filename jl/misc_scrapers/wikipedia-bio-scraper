#!/usr/bin/env python2.4
#
# Tool to scrape journalist bios from wikipedia
# (core wikipedia scraper written by Tom Lynn)
#

import sys
import time
#import textwrap
import traceback
from datetime import datetime
from optparse import OptionParser
import re
import urllib2
import urlparse

sys.path.insert(0, '../pylib')
from JL import ukmedia, Journo, DB
from BeautifulSoup import BeautifulSoup,Tag,NavigableString


_conn = None  # DB connection
_options = None

class NoSuchArticle(Exception):
    pass

class DisambiguationNeeded(Exception):
    '''Reached a disambiguation page'''

class UnsureException(Exception):
    '''May not be the right person'''

#class DeadPersonException(Exception):
#    '''This is an ex-person! Bereft of life, they write no more.'''


def scrape_wikipedia_bio( wikipedia_url ):
    """fetch a wikipedia page and try to scrape a journo bio from it"""
    profile = {}
    bio = {}
    html = ukmedia.FetchURL( wikipedia_url );

    if 'Wikipedia does not have an article with this exact name' in html:
        raise NoSuchArticle("No article with this exact name.")

    if 'Wikipedia:Disambiguation' in html:
        raise DisambiguationNeeded("Disambiguation page")

    OCCUPATIONS = [
        'Journalist', 'Journalism', 'Science_journalist', 'Author', 'Writer',
        'Novelist',
        'Member_of_Parliament',
        'Broadcasting', 'Broadcaster'  # a bit chancy, but excusable
        
        # should we assume sports are sufficient indicators too?
        # there is, after all, only one wikipedia page for all of these names
    ]
    # Check for a definition list at the start
    tables_re = re.compile(r'<table(?=\W).*?</table>', flags=re.DOTALL)
    html2 = '\n'.join(tables_re.split(html))
    tag = re.search(r'<(dd|p)(?=\W)', html2)
    if tag.group(1)=='dd':
        # Check for "For ..., see ...".
        dd = re.search(r'<dd.*?>(.*?)</dd', html2, flags=re.DOTALL).group(1)
        raise UnsureException("Unsure if this is the right person")
        # This test isn't picky enough:
        #
        #for job in OCCUPATIONS:
        #    if job.lower() in dd.lower():
        #        raise Exception("Probably the wrong page")
        #print "WARNING: possibly the wrong page:"
        #print "    %s" % dd.lstrip().encode('utf-8')

    # Any page which uses the word 'journalist' is assumed to be the right one.
    # People related to journalists may fall foul of this, but they're usually
    # journos too :-).  We're careful to miss "see Martin Arnold (journalist)".
    
    if ' journalist' not in html and 'Journalist ' not in html:
        # FIXME: This catches problem pages (inc. disambiguation pages) fine,
        # but we should have a way to force scraping the page.
        
        ok = False
        for word in OCCUPATIONS:
            if ('href="/wiki/%s"' % word) in html:
                ok = True
        if not ok:
            raise UnsureException("No link to writing-occupation page found, may be wrong person.")

    m = re.search(r'<li id="lastmod"> This page was last modified on (.*? at \d\d:\d\d).</li>', html)
    if m is None:
        raise Exception("No modification date.")  # probably no article
#    context['pubdate'] = ukmedia.ParseDateTime(m.group(1))


    # try for an image in the infobox
    soup = BeautifulSoup(html)  # ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ!!!!
    infobox = soup.find('table', {'class': re.compile(r'\binfobox\b')})
    if infobox:
        print "GOT infobox"
        img_link = infobox.find('a', {'class': 'image'})
        if img_link:
            image_page = urlparse.urljoin( wikipedia_url, img_link['href'] )
            image = scrape_image( image_page )
    

    # Extract infobox. Unfortunately BeautifulSoup isn't up to it.
    image = ''
    infobox = None
    m = re.compile('<table class="[^"]*infobox[^"]*?".*?>.*?</table>', re.DOTALL).search(html)
    if m:
        html = html[:m.start()] + html[m.end():]
        infobox = m.group()
        img_re = re.compile(r'<img .*?>', re.DOTALL)
        try:
            image = '<p>%s</p>' % img_re.findall(infobox)[0]
            if 'replace' in image.lower():
                image = ''
        except IndexError:
            image = ''
    
    soup = BeautifulSoup(html)


    keywords = []
    for meta in soup.findAll('meta'):
        attrs = dict(meta.attrs)
        if attrs.get('name')=='keywords' and attrs.get('content'):
            keywords.append(attrs.get('content'))
    bio['keywords'] = ','.join(keywords)
    
    # Remove relative links and [<number>] links.
    for a in soup.findAll('a'):
        contents = a.renderContents(None)
        if contents and re.match(r'\[\d+\]', contents):
            extract_li = False
            if a.parent and a.parent.name == 'li':
                for sub in a.parent.contents:
                    if isinstance(sub, Tag):
                        if sub is not a: break
                        extract_li = True  # a is the first subtags
                    else:
                        if sub.strip() not in ('', '^'): break
            if extract_li:
                a.parent.extract()
            else:
                a.extract()
        elif not a.get('href', '').startswith('http://'):
            replace_with_contents(a)

    # Remove [edit] spans in headings
    for span in soup.findAll('span', {'class': ['editsection', 'mw-headline']}):
        if span['class'] == 'editsection':
            span.extract()
        else:
            span.replaceWith(span.string or '')
    
    # Remove references in the text
    for sup in soup.findAll('sup', attrs={'class': 'reference'}):
        sup.extract()

    # Remove references at the bottom (not reliable)
    for sup in soup.findAll('li', attrs={'id': re.compile('_note.*')}):
        sup.extract()
    
    # Replace their ridiculous use of tables for pull-quotes with blockquotes
    for tag in soup.findAll('table'):
        if dict(tag.attrs).get('class')=='cquote':
            celltext = tag.findAll('td', limit=2)[1].renderContents(None)
            quote = Tag(soup, u'blockquote')
            quote.append(celltext)
            tag.replaceWith(quote)
    
    tags = soup.body.findAll(['h2', 'p', 'ul', 'ol', 'blockquote'])
    
    # blockquotes cause problems - they contain other tags.
    # Remove all tags with a blockquote parent from the list of tags
    # so that we don't break the parent relationships when assigning
    # tags to headings.
    
    for tag in list(tags):
        for parent in get_parents(tag):
            if parent.name=='blockquote':
                tags.remove(tag)

    # Drop the junk after the article (not always present)
    for i, tag in enumerate(tags):
        if tag.name=='ul' and tag.findAll('li', id='ca-nstab-main'):
            break
    tags = tags[:i]

    # Build a list of headings with their sub-tags.
    headings = [(None, [])]
    for tag in tags:
        if tag.name=='h2':
            headings.append((tag, []))
        else:
            headings[-1][1].append(tag)

    def drop_heading(heading):
#        print 'DROPPING:', tagtext(heading)
        return None
    
    def fix_heading(heading, tags):
        '''
        Remove paragraphs with no strings.
        Remove non-special headings that don't start with a paragraph.
        Remove lists from non-special headings.
        '''
        SPECIAL = ['Books', 'Works', 'Bibliography', 'External links',
                   'Further reading']
        tags = [tag for tag in tags if tag is not None and
                    tag.name!='p' or tag.renderContents(None).strip()]
        special = False
        heading_text = tagtext(heading)
        for word in SPECIAL:
            if word.lower() in heading_text.lower():
                special = True
        if heading_text == 'External links and references':
            set_heading_text(heading, 'External links')
        # Shorten lists (even special ones).
        # The motivation is that some pages like to list reams of crap,
        # usually in bibliographies, but in other things too.
        found_lis = 0
        MAX_ITEMS = 10  # per headed section
        for tag in list(tags):
            if tag.name in ('ul', 'ol'):
                for li in tag.findAll('li', recursive=False):
                    found_lis += 1
                    if found_lis > MAX_ITEMS:
                        li.extract()
        # Remove any now-empty uls and ols.
        # Harder than it sounds, due to nested lists.
        temp = Tag(soup, 'p')
        for tag in tags:
            temp.append(tag)
        for tag in temp.findAll(('ul', 'ol')):
            if not tag.findAll(('ul', 'ol', 'li')):
                tag.extract()
        tags = temp.contents
        if found_lis > MAX_ITEMS:
            # Add " (some omitted)" to heading
            if heading_text:
                heading_text = heading_text.replace(' (incomplete)', '')
                if wikipedia_url.startswith('http:'):
                    heading_text += ' (some <a href="%s">omitted</a>)' % wikipedia_url
                else:
                    heading_text += ' (some omitted)'  # no "relative" links
                set_heading_text(heading, heading_text)
        if not special:
            if heading is not None:
                # Remove non-special headings which don't start with a paragraph.
                if not tags or tags[0].name != 'p':
                    return drop_heading(heading)
                # Remove non-special headings containing lists.
                for tag in tags:
                    if tag.name in ('ul', 'ol'):
                        return drop_heading(heading)
            else:
                # Remove lists from None (before first heading, if any).
                tags = [tag for tag in tags if tag.name not in ('ul', 'ol')]
        return (heading, tags)
    
    headings = [fix_heading(*ht) for ht in headings]
    headings = [ht for ht in headings if ht is not None]
    
    # Flatten.
    tags = []
    for heading, htags in headings:
        if heading is not None:
            tags.append(heading)
        # BenC - We're not fussy. Don't mind if they're dead.
#        if tagtext(heading)=='Death':
#            for tag in htags:
#                for year in re.findall(r'19[0-9]\d(?!\d)', tag.renderContents(None)):
#                    raise DeadPersonException("ex-person! Died in %s!" % year)
        tags += htags
    
    # Replace contents heading with an empty paragraph tag.
    # Since it's now the only one, we can use it to detect the
    # end of the introductory paragraphs.
    #
    # Also strip tag contents and stringify.
    
    for i, tag in enumerate(tags):
        if tag.name=='h2' and tagtext(tag)=='Contents':
            tags[i] = u'<p></p>'
        else:
            tagname = unicode(tag.name)
            tags[i] = u'<%s>%s</%s>' % \
                (tagname, strip(tag.renderContents(None)), tagname)
    
    text = '\n'.join(tags)
    
    # Replace ", ." with ".". Arises from our removing cross-references.
    text = re.sub(r',\s*\.', '.', text)
    
    # TODO: image licensing
    profile['image'] = { 'url': image }


#    if image:
#        bio_text = unicode(BeautifulSoup(text).p)  # include <p> tag
#    else:
    bio_text = BeautifulSoup(text).p.renderContents(None)
    bio_text = re.sub('<b>(.*?)</b>', r'\1', bio_text)
    bio['bio'] = bio_text
    bio['kind'] = 'wikipedia-profile'
    bio['srcurl'] = wikipedia_url

    profile['bio'] = bio;
    return profile


## Helpers

def get_parents(tag):
    parents = []
    while tag:
        tag = tag.parent
        if tag: parents.append(tag)
    return parents

def set_heading_text(heading, text):
    while heading.contents:
        heading.contents[-1].extract()
    heading.append(NavigableString(text))

def strip(s):
    '''Strip outer whitespace from s, including <br /> tags.'''
    s = re.search(r'(?s)\s*(?:<br\s*/>\s*)*(.*)', s).group(1)
    return re.search(r'(?s)(.*?)\s*(?:<br\s*/>\s*)*\Z', s).group(1)

def tagtext(tag, strip=True):  # much safer than tag.string
    if tag is None:
        return u''
    text = tag.renderContents(None)
    if strip:
        text = text.strip()
    return text

def replace_with_contents(tag):
    '''Extracts a tag but keeps its contents.'''
    parent, index = tag.parent, tag.parent.contents.index(tag)
    children = tag.contents or []
    tag.extract()
    for child in children:
        parent.insert(index, child)
        index += 1



def find_containing( soup, tag, text ):
    for t in soup.findAll( tag ):
        if t.find( text=text ):
            return t
    return None

def scrape_image( image_page_url ):
    print "SCRAPING ",image_page_url

    html = ukmedia.FetchURL( image_page_url )
    soup = BeautifulSoup( html )

    desc_div = soup.find('div', {'id':'shared-image-desc'} )
    if not desc_div:
        return None

#    license_h2 = find_containing( desc_div, 'h2', text=( "Licensing", "License information") )
#    table = license_h2.findNextSibling( 'table' )

    for foo in desc_div.findAll( 'span', {'class':re.compile(r"\bdescription\b") } ):
        l = foo.renderContents(None).lower()
        licence = None
        if( "GNU Free Documentation License" in l ):
            licence = "GFDL"
        if foo.find( 'a', href=re.compile() )



        l = ukmedia.SanitiseHTML(l)
        print l


    td = desc_div.find( 'th', {'id':"fileinfotpl_desc"} ).findNextSibling('td')
    for cruft in td.findAll( 'span', {'style':re.compile(r"display:\s*none;?")} ):
        cruft.extract()
    desc = td.renderContents(None)
    desc = ukmedia.FromHTMLOneLine( desc )

    td = desc_div.find( 'th', {'id':"fileinfotpl_aut"} ).findNextSibling('td')
    for cruft in td.findAll( 'span', {'style':re.compile(r"display:\s*none;?")} ):
        cruft.extract()
    author = td.renderContents(None)
    author = ukmedia.FromHTMLOneLine( author )

    print "DESC: '%s'" % ( desc )
    print "AUTHOR: '%s'" % ( author )
    return None





# unused - left here for reference if page-discovery code is added back in.
def url_from_prettyname( prettyname ):
        # "Titles on Wikipedia are case sensitive except for the first character"
        foo = prettyname.replace(' ','_')
        return 'http://en.wikipedia.org/wiki/' + urllib2.quote(foo)




def update_bios():
    """ use wikipedia-profile entries in journo_weblinks to drive updates """

    global _conn
    global _options

    c = _conn.cursor()
    if( _options.single_journo ):
        sql = """SELECT l.url, l.journo_id, j.prettyname, l.approved
            FROM journo_weblink l INNER JOIN journo j ON j.id=l.journo_id
            WHERE j.status='a' AND l.kind='wikipedia-profile'
            AND j.ref=%s"""
        c.execute( sql, _options.single_journo  )
    else:
        sql = """SELECT l.url, l.journo_id, j.prettyname, l.approved
            FROM journo_weblink l INNER JOIN journo j ON j.id=l.journo_id
            WHERE j.status='a' AND l.kind='wikipedia-profile'"""
        c.execute( sql )

    good=0
    failed=0
    for row in c.fetchall():
        try:
            if _options.verbose:
                print "processing %s" %(row['url'])
            profile = scrape_wikipedia_bio( row['url'] )
            # if weblink is approved, assume bio is good too (won't affect bios we've already got)
            default_approval = row['approved']
            if 'bio' in profile:
                Journo.load_or_update_bio( _conn, row['journo_id'], profile['bio'], default_approval )
                _conn.commit();
                good = good+1
            else:
                print >>sys.stderr, "WARNING: blank bio %s\n" % (row['url'])

        except Exception,e:
            # always just bail out upon ctrl-c
            if isinstance( e, KeyboardInterrupt ):
                raise
            print >>sys.stderr, "ERROR processing %s\n" % (row['url'])
            print >>sys.stderr, '-'*60
            print >>sys.stderr, traceback.format_exc()
            print >>sys.stderr, '-'*60
            failed = failed+1
            continue

    if _options.verbose:
        print "%d good, %d failed" % (good,failed)





def main(argv=None):
    global _conn
    global _options

    if argv is None:
        argv = sys.argv

    parser = OptionParser()
    parser.add_option( "-j", "--journo", dest="single_journo", help="just do a single journo (eg 'fred-smith')" )
    parser.add_option( "-u", "--url", dest="url", help="just scrape a single profile page and dump to stdout (doesn't touch DB)", metavar="URL" )
    parser.add_option( "-v", "--verbose", action="store_true", dest="verbose", help="output progress information")
    parser.add_option( "-i", "--image", dest="image_url", help="just scrape image page and dump output to stdout", metavar="IMAGEURL" )
    (_options, args) = parser.parse_args()

    if _options.image_url:
        scrape_image( _options.image_url )
    elif _options.url:
        inf = scrape_wikipedia_bio( _options.url )
#        print inf
        return 0
    else:
        _conn = DB.Connect()
        update_bios()


if __name__ == "__main__":
    sys.exit(main())



