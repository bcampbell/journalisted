#!/usr/bin/env python
from __future__ import print_function

usage = """Usage: %prog [options] logfiles..."""

desc = """tool to analyse logfiles and dump out search stats (number of journo searches vs num of article searches)
"""

import re
import operator
import sys
from datetime import datetime,date,timedelta
import os
import fileinput
from optparse import OptionParser

#here = os.path.dirname(__file__)
#import site
#site.addsitedir(os.path.join(here,"../pylib"))
#from JL import DB,ukmedia


#default_log_path = "/srv/vhost/journalisted.com/logs"

HOST=0
REQ=3
REFERER=6
AGENT=7


def main():
    parser = OptionParser(description=desc,usage=usage)

#    parser.add_option("-l", "--logpath",
#        dest="log_path",
#        metavar="LOG_PATH",
#        help="location of logfiles",
#        default=default_log_path)

    (options, args) = parser.parse_args()

    logfiles = args
    if len(logfiles)==0:
        print('ERROR: no logfiles specified.', file=sys.stderr)
        sys.exit(1)


    # default time range is last 7 days
#    end = datetime.utcnow()
#    start = end - timedelta(days=7)

    src = fileinput.input(logfiles,openhook=fileinput.hook_compressed)
    foo = parse_server_log(src)

    # searches only
    foo = (l for l in foo if 'GET /search' in l[REQ])

    # filter out likely bots
    foo = (l for l in foo if l[REFERER]!="-")
    foo = (l for l in foo if 'bot' not in l[AGENT].lower())


    journo_cnt = 0
    art_cnt=0

    for l in foo:
        req = l[REQ]
        if "type=journo" in req or "?j=" in req or "&j=" in req:
            journo_cnt += 1
        elif "type=article" in req or "?a=" in req or "&a=" in req:
            art_cnt += 1

    print( journo_cnt, "journo searches")
    print( art_cnt, "article searches")

#    hits = analyse(src)
#    for ref,cnt in hits:
#        print("%s,%s" % (cnt,ref) )





def parse_server_log(raw_lines):
    """ convert raw log file lines into (host,time,req) tuples """

    # from http://www.seehuhn.de/blog/52
    parts = [
        r'(?P<host>\S+)',                   # host %h
        r'\S+',                             # indent %l (unused)
        r'(?P<user>\S+)',                   # user %u
        r'\[(?P<time>.+)\]',                # time %t
        r'"(?P<request>.+)"',               # request "%r"
        r'(?P<status>[0-9]+)',              # status %>s
        r'(?P<size>\S+)',                   # size %b (careful, can be '-')
        r'"(?P<referer>.*)"',               # referer "%{Referer}i"
        r'"(?P<agent>.*)"',                 # user agent "%{User-agent}i"
    ]
    # eg
    # 66.249.75.112 - - [27/May/2013:00:00:00 +0100] "GET /qin-xie/rss HTTP/1.1" 200 3560 "-" "Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_1 like Mac OS X; en-us) AppleWebKit/532.9 (KHTML, like Gecko) Version/4.0.5 Mobile/8B117 Safari/6531.22.7 (compatible; Googlebot-Mobile/2.1; +http://www.google.com/bot.html)"

    log_cracker = re.compile(r'\s+'.join(parts)+r'\s*\Z')

    for line in raw_lines:
        m = log_cracker.match(line)
        assert m is not None
        yield (m.group('host'),
                m.group('user'),
                m.group('time'),
                m.group('request'),
                m.group('status'),
                m.group('size'),
                m.group('referer'),
                m.group('agent'))


def parse_time(inp):
    # my strptime doesn't support %z, so just strip the timezone
    strip_tz = re.compile(r" [+-]([0-9])+$")
    for (host,t,req) in inp:
        t = strip_tz.sub("",t)
        dt=datetime.strptime(t,"%d/%b/%Y:%H:%M:%S")
        yield (host,dt,req)

def only_between(inp,start,end):
    for (host,t,req) in inp:
        if t>=start and t<end:
            yield (host,t,req)


if __name__ == "__main__":
    main()

