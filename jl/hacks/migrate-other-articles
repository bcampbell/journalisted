#!/usr/bin/env python
#
# migrate articles from "journo_other_articles" to the main "article" table
# sideeffect: creates new publications.
#


import sys
import string
from datetime import datetime
from optparse import OptionParser
import urlparse
import re

import site
site.addsitedir("../pylib")
from JL import DB

__opts = None
_conn = None



class UnresolvedPublication(Exception):
    pass


def resolve_publication( domain, name ):
    """ look up a publications, return publication id or None """



    # use domain to look them up
    # want to look for both www. and bare versions
    domain = domain.lower().strip().encode( 'ascii' )
    candidates = [ domain ]
    if domain.startswith( 'www.' ):
        candidates.append( re.sub( '^www.','',domain ) )
    else:
        candidates.append( 'www.' + domain )

    c = _conn.cursor()
    c.execute( "SELECT pub_id FROM pub_domain WHERE domain in ( %s,%s )",
        (candidates[0], candidates[1]) )
    matched_domains = [ row['pub_id'] for row in c.fetchall() ]

    if len( matched_domains ) == 1:
        # got it!
        return matched_domains[0]

    elif len( matched_domains ) == 0:
        # no matching domain - try looking up by name instead
        n = name.strip().encode( 'utf-8' )
        c.execute( """SELECT pub_id FROM pub_alias a WHERE LOWER(alias)=LOWER(%s)""", (n,) )
        matched_names = [ row['pub_id'] for row in c.fetchall() ]
        if len(matched_names) == 0:
            return None     # give up
        if len(matched_names) == 1:
            return matched_names[0]
        raise UnresolvedPublication( "Can't disambiguate publication (domain: '%s' name: '%s') - no domains, but multiple names" % (domain,name) )

    elif len( matched_domains ) > 1:
        # more than one matching domain - try to disambiguate using name
        n = name.strip().encode( 'utf-8' )
        if n== '':
            return matched_domains[0]   # no name - just assume first match.

        sql = """SELECT pub_id FROM pub_alias WHERE LOWER(alias)=LOWER(%s) AND pub_id IN ( SELECT pub_id FROM pub_domain WHERE domain IN (%s,%s) )"""
        c.execute( sql, (n,candidates[0], candidates[1]) )
        matched_names = [ row['pub_id'] for row in c.fetchall() ]
        if len(matched_names) == 1:
            return matched_names[0]
        if len(matched_names) == 0:
            raise UnresolvedPublication( "Can't disambiguate publication (domain: '%s' name: '%s') - multiple domains, no names" % (domain,name) )
        if len(matched_names) > 1:
            raise UnresolvedPublication( "Can't disambiguate publication (domain: '%s' name: '%s') - multiple domains, multiple names" % (domain,name) )

    assert False    # shouldn't get this far




def create_publication( domain, publication ):
    if publication.strip() == u'':
        # use domain for missing publication names
        publication = unicode( domain )
        publication = re.sub( u'^www.',u'',publication )

    c = _conn.cursor()
    c.execute( """INSERT INTO organisation (id,shortname,prettyname,home_url) VALUES (DEFAULT, %s,%s,%s) RETURNING id""",
        ( domain, publication, "http://" + domain ) )
    pub_id = c.fetchone()[0]

    c.execute( """INSERT INTO pub_domain (pub_id,domain) VALUES (%s,%s)""", (pub_id,domain) )
    c.execute( """INSERT INTO pub_alias (pub_id,alias) VALUES (%s,%s)""", (pub_id,publication) )

    if _opts.verbose:
        print "new publication [%d]: %s (%s)" % ( pub_id,publication.encode('utf-8'),domain.encode('utf-8'))
    return pub_id



def migrate_article( art ):
    # already got it?
    cursor = _conn.cursor()
    srcid = art['permalink']
    cursor.execute( "SELECT id FROM article WHERE srcid=%s", srcid )

    existing = cursor.fetchall()
    if len(existing) > 0:
        print >>sys.stderr, "already got %s - SKIPPING." % (art['permalink'].encode('utf-8'))
        return

    # 1) sort out publication (create if necessary)
    url = art['permalink'].strip()


    # some domains people have entered which we don't accept
    domain_blacklist = ( 'factiva.com', 'tinyurl.com', 'findarticles.com', 'bit.ly', 'visiolink.com', 'youtube.com',
        'lexisnexis.com', 'google.com', 'feedproxy.google.com', 'feeds.feedburner.com', 'rss.mediafed.com' )


    o = urlparse.urlparse(url)
    domain = o[1]
    domain = domain.lower()
    if domain == '':
        print >>sys.stderr, "SKIP bad/blank url (other_id=%s)" % (art['other_article_id'])
        return
    if domain in domain_blacklist:
        print >>sys.stderr, "SKIP blacklisted url (other_id=%s): %s" % (art['other_article_id'], url.encode('utf-8') )
        return

    try:
        srcorg = resolve_publication( domain, art['publication'] )
    except UnresolvedPublication,e:
        print >>sys.stderr, "SKIP article (other_id=%s): %s" % (art['other_article_id'],e)
        return

    if srcorg is None:
        srcorg = create_publication( domain,art['publication'] )

    # 2) add article
    # send text to the DB as utf-8
    title = art['title'].encode( 'utf-8' )
    byline = u''
    description = u''
    pubdate = "%s" %(art['pubdate'])
    lastscraped = None
    lastseen = datetime.now()
    firstseen = lastseen
    srcurl = art['permalink']
    permalink = art['permalink']
    wordcount = None


    q = """INSERT INTO article (id,title, byline, description, lastscraped, pubdate, firstseen, lastseen, permalink, srcurl, srcorg, srcid, wordcount, last_comment_check) VALUES (DEFAULT, %s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s) RETURNING id"""
    cursor.execute( q, ( title, byline, description, lastscraped, pubdate, firstseen, lastseen, permalink, srcurl, srcorg, srcid, wordcount, lastscraped ) )
    article_id = cursor.fetchone()[0]

    # 3) attribute journo
    cursor.execute( "INSERT INTO journo_attr ( journo_id, article_id) VALUES (%s,%s)", (art['journo_id'],article_id) )

    # 4) mark article for indexing
    cursor.execute( "DELETE FROM article_needs_indexing WHERE article_id=%s", (article_id) )
    cursor.execute( "INSERT INTO article_needs_indexing (article_id) VALUES (%s)", (article_id) )

    if _opts.verbose:
        print "a%d: %s" % ( article_id, art['permalink'].encode('utf-8') )

    # 5) delete the redundant other_article
    cursor.execute( "DELETE FROM journo_other_articles WHERE id=%s", (art['other_article_id']) )



def migrate_articles():
    c = _conn.cursor()
    c.execute( """SELECT * FROM journo_other_articles WHERE status='a'""" )
    while 1:
        row = c.fetchone()
        if row is None:
            break
        other_art = {
            'other_article_id': int( row['id'] ),
            'permalink': row['url'].decode('utf-8'),
            'title': row['title'].decode('utf-8'),
            'pubdate': row['pubdate'],
            'publication': row['publication'].decode('utf-8'),
            'journo_id': int( row['journo_id'] )
        }
        try:
            migrate_article( other_art )
        except Exception, e:
            print >>sys.stderr, "EXCEPTION (other_id=%s): %s" % (other_art['other_article_id'],e)






def main():
    global _opts,_conn

    parser = OptionParser()
    parser.add_option("-v", "--verbose", action="store_true", dest="verbose", help="output more (by default, only suspect stuff of output)")
    (_opts, args) = parser.parse_args()
    _conn = DB.Connect()

    migrate_articles()
    _conn.commit()

#    c = _conn.cursor()
#    c.execute( "SELECT id,prettyname,home_url FROM organisation ORDER by prettyname" )
#    orgs = c.fetchall()
#    for row in orgs:
#        print row['id'],row['prettyname'],row['home_url']
#    print len(orgs), "orgs"
#    c.execute( "SELECT status,count(*) as cnt from journo_other_articles group by status" )
#    for row in c.fetchall():
#        print row['status'],row['cnt']
#    _conn.rollback()




if __name__ == "__main__":
    main()

