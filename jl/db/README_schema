Journa-list database Schema
===========================

Tables
------

alert                 - My Journa-list alerts
article               - the articles (the core of everything)
article_dupe          - info about duplicate articles (might drop this)
article_tag           - tags (aka terms, subjects) extracted from articles
article_commentlink
article_bloglink
article_content
article_url           - known urls for an article
article_image
article_needs_indexing
article_tag
article_similar
article_error

journo                - the journalists
journo_alias          - names for journalists (to handle mispelllings etc) 
journo_attr           - links articles to journalists (attribution)
journo_average_cache  - cache for journo stats
journo_jobtitle       - job titles found for journos
journo_weblink        - links to journo websites (wikipedia etc)
journo_books
journo_email
journo_education
journo_other_articles
journo_employment
journo_phone
journo_jobtitle
journo_photo
journo_bio
journo_attr
journo_awards
journo_similar
journo_alias
journo_admired
journo_address
journo_weblink
journo_pageviews      - pageviews for the last 7 days, calculated from logfiles (see bin/update_journo_pageviews)

organisation          - the organisations in the system (newspapers, bbc etc)
pub_adr
pub_alias
pub_domain
pub_email_format
pub_phone
pub_set_map
pub_set

htmlcache             - used to speed up some pages by caching bits of html
person                - user accounts (for my journa-list)
person_receives_newsletter
requeststash          -
secret                -
tag_blacklist         - tags to ignore during term extraction
tag_synonym           - to tie multiple tags together
token                 -



custompaper
custompaper_criteria_text
custompaper_criteria_journo

event_log
error_articlescrape
image
htmlcache
news
person_permission
missing_articles
link
requeststash
secret
token
recently_viewed




The article table is the core of the whole system. All the scraped articles
are stored there, and most other data in the system is derived from them.


article table
-------------


id (integer, primary key, not null)
  The unique id for the article.


title (text, not null)
  The article headline. Single line, utf-8, plain text.


byline (text, not null)
 The article byline as scraped (eg "By Fred Smith, Washington
 and Fred Bloggs, Political Correspondent"). Single line, utf-8,
 plain text.


description (text, not null)
 Short description of the article (utf-8, plain text)


pubdate (timestamp)
 When the article was published (or when the article claims it was published).
Different outlets do things a bit differently. For example, one paper might
only have dates, so all the articles will appear to show up at midnight.
It's a bit rough.


firstseen (timestamp, not null)
 When the article is first seen by the scraper. The idea is that firstseen
and lastseen can track the period the article was active, as the scrapers
are run regularly.
For historical archives, it's probably better for firstseen/lastseen to be
set to the pubdate, rather than the time they were scraped.


lastseen (timestamp, not null)
 The last time the article was seen by the scrapers.
Currently the scrapers don't correctly update this after the initial scrape.
But the idea is that this'll be updated so that we get an idea of how long
the article was active on the source site...



permalink (text, not null)
 URL to original article. Might be different to the one scraped (eg
we might find it easier to scrape the printer version of an article).
This is the URL we serve up to the user when they want to read the
original article.

srcurl (text, not null)
 The original url which we scraped the article from. Usually the same as
permalink, but doesn't have to be.

srcorg (integer, not null)
 The ID of the organisation the article came from (foreign key - indexes
into the "organisation" table).

srcid (text, not null)
 (DEPRECATED - now have multiple article_url entries instead)
 A unique ID which identifies the article within database.
 (uniqueness not yet enforced in the database, but might be in future)
 Often sites can have multiple urls which refer to the same article.
 Which is a pain.
 But luckily, sites usually expose some kind of uniqueness in the URL,
 for example, their internal database IDs.
 eg:
 http://news.independent.co.uk/uk/politics/article3061121.ece
 For this independent url, it's pretty obvious that 3061121 is going to
 be unique.
 srcid generation is pretty newspaper-specific, but must be unique. To
 ensure this, most srcids start with the name of the newspaper
 eg "independent_article3061121.ece"

lastscraped (timestamp)
 When the article was last scraped

wordcount (integer)
 Number of words in the article.

status ( character(1), default 'a' )
 'a'=active
 Any other value means article is hidden and not included in analysis.
 'h'=hidden
 'd'=duplicate

total_bloglinks (integer)
total_comments (integer)
 These two track the total number of blog reference and comments, stored
 in the article_bloglink and article_commentlink tables respectively.
 They are automatically updated via triggers when those other two tables
 are modified.

last_comment_check (timestamp)
 Last time at which the article was checked for comments on it's home site
 (many papers allow readers to post comments).
 This allows us to rescrape the comment counts to keep them up to date.


article_url table
-----------------

article_id (integer)
url (text)

Articles very often have multiple urls. This table collects all the ones
we know about. These might come via HTTP redirects, rel-canonical links
in the html etc etc...




article_content table
---------------------

article_id (integer)

content (text)
 The actual article text (utf-8). Uses a very cut-down set of HTML, with
very basic formatting (<p>, <br>, <strong>, headings etc). The scrapers
should remove most style fluff and just leave the text and it's basic
structure.

scraped (timestamp)
 When the content was scraped


journo table
------------

id (primary key)
 unique id for the journo

ref (text)
 A unique string which identifies the journo. Used as the url for the
 journos webpage - ie http://journalisted.com/{ref}
 By convention, the ref is a lowercase version of the name (without any
 title prefixes or postfixes), with spaces replaced by hyphens.
 eg "Fred Smith" becomes "fred-smith"
 Also, any non-url-safe characters (eg accented european character)
 should be replaced by safe ones.
 Note: * ref can be changed! *
 When multiple journos are discovered with the same name, the convention
 is to split them by adding a number to the end:
 eg "fred-smith" is split into "fred-smith-1" and "fred-smith-2" etc..
 The original 'fred-smith' is left unused, so that if someone types it
 in as a url we can display a disambiguation/search page.

prettyname (text)
 Full display name (eg "Professor Fred J Smith MP")

lastname (text)
 Surname of journo, lowercase (eg "smith")

firstname (text)
 First name of journo lowercase (eg "fred")

created (timestamp)
 When the journo was first entered into the db.

status (character(1))
 single character
 'i' - inactive (journos start off in this state, and are not shown on the
       site. They are automatically activated when they have 2 or more
       articles.)
 'a' - active (normal, visible state)
 'h' - hidden (manually marked as hidden - for whatever reason, we don't
       want this journo displayed (it might be a byline mistake, eg
       "Liverpool Forward").



person_permission table
-----------------------

id (integer)
    unique row id
person_id (integer, not null)
    foreign key into person table
journo_id
    foreign key into journo table, or NULL
    the specific journo on which this permission is granted, if any
permission (text, not null)
    'edit' - user can edit specified journos profile
    'claimed' - user has claimed editing rights on journo, but not yet approved
    'deny' - claim was denied on journo
    'admin' - godlike do-anything status :-)
reated
    timestamp for when this permission was created

