#!/usr/bin/env python

# tool to analyse logfiles for the last week and output a list of journos,
# sorted by number of hits.
#
# 

from __future__ import print_function
import re
import operator
import sys
import datetime
import os
import fileinput
from optparse import OptionParser

default_log_path = "/srv/vhost/journalisted.com/logs"


# from http://www.seehuhn.de/blog/52
parts = [
    r'(?P<host>\S+)',                   # host %h
    r'\S+',                             # indent %l (unused)
    r'(?P<user>\S+)',                   # user %u
    r'\[(?P<time>.+)\]',                # time %t
    r'"(?P<request>.+)"',               # request "%r"
    r'(?P<status>[0-9]+)',              # status %>s
    r'(?P<size>\S+)',                   # size %b (careful, can be '-')
    r'"(?P<referer>.*)"',               # referer "%{Referer}i"
    r'"(?P<agent>.*)"',                 # user agent "%{User-agent}i"
]
# eg 66.249.75.112 - - [27/May/2013:00:00:00 +0100] "GET /qin-xie/rss HTTP/1.1" 200 3560 "-" "Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_1 like Mac OS X; en-us) AppleWebKit/532.9 (KHTML, like Gecko) Version/4.0.5 Mobile/8B117 Safari/6531.22.7 (compatible; Googlebot-Mobile/2.1; +http://www.google.com/bot.html)"

log_cracker = re.compile(r'\s+'.join(parts)+r'\s*\Z')

# pattern to extract a journo ref from a GET request
journo_pat = re.compile("GET /([a-z]+[-][-a-z0-9]+).*",re.I)

# reqs that look like journos but aren't.
backlist_journos = ('apple-touch-icon-precomposed', 'apple-touch-icon')

def main():
    # first pass: collect all unique (ipaddress,journo) pairs
    # this culls out multiple hits from same ip address
    # and merges all related access eg: /bob-smith and /bob-smith/rss
    # accessed from the same IP address count as single hit
    # TODO: take timestamp into account? Or just keep relying on
    # per-day logfiles...

    parser = OptionParser()
    parser.add_option("-l", "--logpath",
        dest="log_path",
        metavar="LOG_PATH",
        help="location of logfiles",
        default=default_log_path)

    (options, args) = parser.parse_args()

    logfiles = get_log_list(options.log_path)
    if len(logfiles)==0:
        print('ERROR: no logfiles found.', file=sys.stderr)
        sys.exit(1)

    src = fileinput.input(logfiles)
    hits = analyse(src)
    for ref,cnt in hits:
        print("%s,%s" % (cnt,ref) )



def get_log_list(log_dir):
    """ get a list of logfiles from the past week """
    logs = []
    today = datetime.date.today()
    for i in range(7):
        d = today - datetime.timedelta(days=i)
        filename = os.path.join(log_dir,"access_log.%s"%(d.strftime("%Y%m%d")))
        if os.path.isfile(filename):
            logs.append(filename)
        else:
            print('WARNING: %s not found'%(filename,), file=sys.stderr)
    return logs


def analyse(infile):
    accesses = set()
    
    for line in infile:
        m = log_cracker.match(line)
        assert m is not None
        host= m.group('host')
        req = m.group('request')

        # just look for journos
        m = journo_pat.match(req)
        if m is not None:
            journo_ref = m.group(1)
            if journo_ref not in backlist_journos:
                key = (host,m.group(1))
                accesses.add((host,journo_ref))

    # pass two: count how many hits to each journo
    counts = {}
    for (host,req) in accesses:
        counts[req] = counts.get(req,0) + 1

    # pass three: sort the results (into a list of (journo_ref,hitcount) tuples)
    hits = sorted(counts.iteritems(), key=operator.itemgetter(1), reverse=True)

    return hits



if __name__ == "__main__":
    main()
